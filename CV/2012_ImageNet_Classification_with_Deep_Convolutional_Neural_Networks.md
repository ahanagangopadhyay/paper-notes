You can find the paper [here](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf).

![#1589F0](https://placehold.it/15/1589F0/000000?text=+) **Significance**

With well over 50,000 citations till date, this 2012 paper by Geoffrey Hinton's group at University of Toronto is a seminal paper in computer vision and regarded as one of the papers that established Convolutional Neural Networks in the forefront for image classification tasks. A variant of the deep CNN proposed here was entered in the ILSVRC (ImageNet Large-Scale Visual Recognition Challenge) 2012, which won with a top-5 error rate of 15.3%, outperforming the second-place winner (26.2%) by a large margin.

![#1589F0](https://placehold.it/15/1589F0/000000?text=+) **Summary**

This paper came at a time when two important (and very fortunate) things happened almost at the same time: large, labeled datasets consisting of millions of images became available for feeding data-hungry giant networks; and powerful GPUs capable of running such large-scale networks also became available. The authors trained one of the largest CNNs of that time on the subsets of ImageNet dataset that were used in 2010 and 2012 competitions using a highly-optimized GPU implementation of 2D convolution; and experimented with several techniques to improve performance, reduce training time and reduce overfitting. Their experiments suggested that these results can be improved upon by simply waiting for faster GPUs and bigger datasets.

![#1589F0](https://placehold.it/15/1589F0/000000?text=+) **Dataset**

The ImageNet dataset consists of 15 million labeled high-resolution images from 22,000 categories, collected from the web and labeled manually. The ILSVRC challenge uses a subset of ImageNet with roughly 1000 images from each of 1000 categories, summing up to about 1.2 million training images, 50,000 validation images and 150,000 test images. The images have variable resolution, so they were first downsampled to a fixed resolution of 256x256 and the mean activity over the entire training set was subtracted from each pixel.

![#1589F0](https://placehold.it/15/1589F0/000000?text=+) **Proposed Approach**

The CNN used in the paper had 5 convolutional and 3 fully-connected layers. Interestingly, the authors found that removing any convolutional layer (each of which has <1% of the model's parameters) resulted in a deterioration in performance. The characteristic features of the proposed architecture are listed below in the order of importance:
- **ReLU nonlinearity:** Training with gradient descent on saturating neurons typically takes much more time than training on non-saturating nonlinearities like ReLU (f(x) = max(0, x)). So ReLU was a natural choice for training such a large network on a large dataset.
- **Training on multiple GPUs:** Memory size of a single GPU limits the maximum network size that can be trained on it. The authors spread the network over 2 GPUs and tuned the architecture so that cross-GPU communication was an acceptable fraction of the total amount of computation. Half of the kernels (neurons) were put on each GPU, and additionally, the GPUs only communicated in certain layers while taking inputs from kernels residing on the same GPU for other layers. They note that compared to a network with half as many kernels in each conv layer trained on one GPU, the 2-GPU net reduces top-1 and top-5 error rates by 1.7% and 1.2% respectively.
- **Local response normalization:** Although ReLUs, unlike saturating neurons, do not require input normalization to prevent them from saturating, the authors found that normalizing the activity of a neuron in a kernel using the activities of neurons at the same spatial location in adjacent kernel maps reduced their top-1 and top-5 errors by 1.4% and 1.2% respectively.
- **Overlapping pooling:** The authors found that overlapping neighborhoods summarized by the pooling layers reduced the top-1 and top-5 error rates by 0.4% and 0.3% respectively, compared to pooling over non-overlapping neighborhoods of neurons.

The ReLU nonlinearity was applied to the output of every conv and fully-connected layer, whereas response normalization followed the first two conv layers. Max-pooling followed conv layers 1, 2 and 5. Kernels of conv layers 2, 4 and 5 were connected only to those kernel maps in the previous layer that resided on the same GPU, whereas kernels of conv layer 3 were connected to all kernel maps in conv layer 2. The 5 conv layers were parameterized in order as: 96 kernels (11x11x3, stride=4), 256 kernels (5x5x48), 384 kernels (3x3x256), 384 kernels (3x3x192) and 256 kernels (3x3x192). The fully-connected layers consisted of 4096 neurons each. The output of the last fully-connected layer is fed to a 1000-dimensional softmax layer which determines a distribution over the 1000 class labels. 

Even with 1.2 million training images, this large-scale network with 60 million parameters is prone to overfitting. The authors provide the following techniques they implemented to avoid overfitting:
- **Data augmentation:** Label-preserving image transformations were generated on the CPU while GPU training was done on the previous batch of images, so that transformed images were not needed to store on disk. One form of data augmentation involved generating image translations and horizontal reflections. Random 224x224 image patches and their horizontal reflections were extracted from training images and added to the training set, largely reducing the generalization error. During testing, the network made a prediction by extracting 5 224x224 patches (4 corner patches and the center patch) and their reflections (thus, 10 in total), and averaged the predictions made by the softmax layer on the 10 patches. The second form of data augmentation implements alteration of image intensities of the RGB channels in the training images in order to capture the invariance of object identity with changes in intensity and color of illumination. Specifically, the authors added, to each training image, multiples of PCA components of the RGB pixel values, with magnitudes corresponding to the corresponding eigenvalues times a random variable that is drawn from a Gaussian distribution only once for all pixels of a training image, and re-drawn when that image is used for training again. This scheme reduced the top-1 error rate by more than 1%.
- **Dropout:** The authors implemeted the dropout technique where the output of each hidden neuron was randomly set to zero with a probability of 0.5 in each pass. Thus with every input, the network samples a different architecture, but all the architectures share weights. Dropouts were used in the first two fully-connected layers, substantially reducing overfitting by making the neurons learn robust features since no neuron can completely rely on the presence of other neurons to learn complex co-adpatations. At test time, all neurons were used, and their outputs were multiplied by 0.5.

Training was done using stochastic gradient descent with a batch size of 128, momentum of 0.9 and a weight decay of 0.0005. Learning rate was initialized at 0.01 for all layers, and divided by 10 three times, each when the validation error rate stopped improving with the current learning rate. The network was trained for 90 epochs on the training set.

![#1589F0](https://placehold.it/15/1589F0/000000?text=+) **Results**
The best performance on ILSVRC-2010 challenge (for which the test labels are available) was top-1 and top-5 error rates of 47.1% and 28.2% respectively using an ensemble of sparse-coding models. Since then, the best published results were 45.7% and 25.7% using an ensemble of classifiers trained on Fisher Vectors of features. The results using the proposed deep CNN produced an impressive improvement over these, with error rates of 37.5% and 17.0%. Their best result on the ILSRVC-2012 challenge (top-5 error rate of 15.3%) similarly outperforms the second best result (26.2%) by a large margin. The authors also report the performance of their network on the Fall 2009 version of ImageNet with 8.9 million images and 10,184 categories, for which the top-1 and top-5 error rates are 67.4% and 40.9% compared to the previous bests of 78.1% and 60.9%.

![#1589F0](https://placehold.it/15/1589F0/000000?text=+) **Discussions**
The convolutional kernels learned by conv layer 1 of the network shows that the data-restricted connectivity of the 2 GPUs gives rise to interesting specializations exhibited by each of them: GPU-1 kernels are largely color-agnostic while GPU-2 kernels are largely color-specific. As a way of validating the visual knowledge learned by the network, the authors compare a set of test images with others from the training set which have the smallest Euclidean distance from the test image using the
4096-dimensional activations of the last hidden layer. It can be seen that although at the raw pizel level, the retrieved training images are not necessarily close to the the test image, the higher levels of the network are indeed able to extract the visual information from them to cluster similar images together. Removal of even one conv layer resulted in a loss of about 2% in the top-error rate, suggesting that the network depth played an important role in the performance.
