You can find the paper [here](http://proceedings.mlr.press/v15/glorot11a/glorot11a.pdf).


![#1589F0](https://placehold.it/15/1589F0/000000?text=+) **Significance**

This 2011 paper by Yoshua Bengio's group at the University of Montreal outlined the benefits of using Rectified Linear Units (ReLUs) for deep but fully supervised networks. Neural networks trained with logistic sigmoid or hyperbolic tangent neurons were hard to train in a fully supervised fashion when the number of layers were too large, requiring semi-supervised or unsupervised pre-training to bootstrap performance. Although rectifier neural networks can take advantage of semi-supervised or unsupervised setups, the authors show that these networks achieve their best performance in purely supervised setups with large quantities of labeled data. The performance is also as good as or even better than networks trained with sigmoid or tanh neurons. This result decreases the performance gap between deep networks trained with and without unsupervised pre-training. 

![#1589F0](https://placehold.it/15/1589F0/000000?text=+) **Summary**

The authors argue that rectifier units are closer to a biological neuron's response in their main operating regime when compared to sigmoid or tanh neurons, bridging - in a sense - the gap between machine learning and neuroscience. In biological networks, only around 1-4% of the total number of neurons are active at any time, leading to a naturally sparse representation. Without imposing additional L1 regularization, feedforward networks do not have this property. In fact sigmoid neurons have a steady-state around 0.5, so that on an average, neurons fire at half their saturation, making it biologically implausible. Tanh neurons have a steady state at 0, making them better in terms of optimization, but they have an antisymmetry around 0 which is also biologically implausible.

Activation functions that plot expected firing rate as a function of total input in models of biological neurons, on the other hand, are typically one-sided, having zero response below a threshold, and a slowly saturating one above threshold. A rectifying non-linearity also produces a one-sided response, aligning more with biological activation functions and giving rise to truly sparse representations. Why is such a representation important?
- A dense representation is highly entangled, because any change in inputs modifies most of the elements in the representation vector. For a sparse and robust representation, the set of non-zero features is more or less conserved by small changes in the inputs.
- Unlike a dense representation where all the neurons will be active most of the time, here we have an additional control to change the number of active neurons to indicate different information contents that different inputs might have.
- Sparse representations are more likely to be linearly separable because the information is represented in a much higher dimensional space, which means we would need less nonlinear machinery to distinguish different classes. Also raw data is often sparse in nature, and we can exploit this inherent sparsity through this type of representation.
- The efficiency of sparse representations is much greater than their dense counterparts, with the power of the exponent being the number of non-zero features.
