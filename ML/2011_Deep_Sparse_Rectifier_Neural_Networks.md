You can find the paper [here](http://proceedings.mlr.press/v15/glorot11a/glorot11a.pdf).


![#1589F0](https://placehold.it/15/1589F0/000000?text=+) **Significance**

This 2011 paper by Yoshua Bengio's group at the University of Montreal outlined the benefits of using Rectified Linear Units (ReLUs) for deep but fully supervised networks. Neural networks trained with logistic sigmoid or hyperbolic tangent neurons were hard to train in a fully supervised fashion when the number of layers were too large, requiring semi-supervised or unsupervised pre-training to bootstrap performance. Although rectifier neural networks can take advantage of semi-supervised or unsupervised setups, the authors show that these networks perform as well as or even better than sigmoid or tanh networks in purely supervised setups with large quantities of labeled data.
