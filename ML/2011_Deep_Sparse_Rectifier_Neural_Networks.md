You can find the paper [here](http://proceedings.mlr.press/v15/glorot11a/glorot11a.pdf).


![#1589F0](https://placehold.it/15/1589F0/000000?text=+) **Significance**

This 2011 paper by Yoshua Bengio's group at the University of Montreal outlined the benefits of using Rectified Linear Units (ReLUs) for deep but fully supervised networks. Neural networks trained with logistic sigmoid or hyperbolic tangent neurons were hard to train in a fully supervised fashion when the number of layers were too large, requiring semi-supervised or unsupervised pre-training to bootstrap performance. Although rectifier neural networks can take advantage of semi-supervised or unsupervised setups, the authors show that these networks achieve their best performance in purely supervised setups with large quantities of labeled data. The performance is also as good as or even better than networks trained with sigmoid or tanh neurons. This result decreases the performance gap between deep networks trained with and without unsupervised pre-training. 

![#1589F0](https://placehold.it/15/1589F0/000000?text=+) **Summary**

The authors argue that rectifier units are closer to a biological neuron's response in their main operating regime when compared to sigmoid or tanh neurons, bridging - in a sense - the gap between machine learning and neuroscience. In biological networks, only around 1-4% of the total number of neurons are active at any time, leading to a naturally sparse representation. Without imposing additional L1 regularization, feedforward networks do not have this property. In fact sigmoid neurons have a steady-state around 0.5, so that on an average, neurons fire at half their saturation, making it biologically implausible. Tanh neurons have a steady state at 0, making them better in terms of optimization, but they have an antisymmetry around 0 which is also biologically implausible.

Activation functions that plot expected firing rate as a function of total input in models of biological neurons, on the other hand, are typically one-sided, having zero response below a threshold, and a slowly saturating one above threshold. A rectifying non-linearity also produces a one-sided response, aligning more with biological activation functions and giving rise to truly sparse representations. Why is such a representation important?
- A dense representation is highly entangled, because any change in inputs modifies most of the elements in the representation vector. For a sparse and robust representation, the set of non-zero features is more or less conserved by small changes in the inputs.
- Unlike a dense representation where all the neurons will be active most of the time, here we have an additional control to change the number of active neurons to indicate different information contents that different inputs might have.
- Sparse representations are more likely to be linearly separable because the information is represented in a much higher dimensional space, which means we would need less nonlinear machinery to distinguish different classes. Also raw data is often sparse in nature, and we can exploit this inherent sparsity through this type of representation.
- The efficiency of sparse representations is much greater than their dense counterparts, with the power of the exponent being the number of non-zero features.

The rectifier activation function rectifier(x) = max(0, x) easily produces a sparse representation, with around 50% of hidden neuron outputs being real zeros upon uniform initialization of weights. Sparsity-inducing regularizations can increase this percentage even further. Very importantly, the only non-linearity in the network comes from the path selection associated with active neurons, and not from the input-output mapping. Once this subset of active neurons is located, the output is a simple linear function of the input. As a result of this linearity, gradients flow well on the active paths of the neurons, eliminating gradient vanishing effects due to activation non-linearities like sigmoid, tanh, etc. Computations are also simpler since we do not need to compute exponential non-linearities.

To see if the hard saturation at 0 hurts optimization by blocking gradient back-propagation, the authors also investigated the softplus activation softplus(x) = log(1+e^x), a smoother version of the rectifying non-linearity, to see if it is easier to train. However surprisingly, experimental results showed that relu works better, indicating that hard zeros can actually help supervised training, as long as some of the hidden neurons in each layer are active such that the gradients have some path to flow.

A potential problem with ReLUs is that activations may be unbounded, since there is no saturation nonlinearity. So the authors used L_1 penalty on the activation values, which made the network even more sparse.

Unsupervised pre-training using stacked auto-encoders had previously been found to be very useful in training deep networks. But there were some problems when rectifier activations were introduced in stacked denoising auto-encoders. Due to the hard zero below threshold, when the network mistakenly happened to reconstruct a zero in place of a non-zero target, the reconstruction unit could not backpropagate any gradient to rectify the mistake. Some of the strategies used by the authors to work around this problem include using softplus activation for the reconstruction layer along with a quadratic cost (works well for image data); and scaling rectifier activations in encoding layers to [0, 1] followed by sigmoid activation and cross-entropy cost for reconstruction (works well for text data).
