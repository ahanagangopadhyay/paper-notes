In order to train multilayer neural networks, it is necessary to figure out what part of the prediction error at the output should be assigned to which neuron - also called the problem of credit assignment. A huge breakthrough in the field of machine learning came by way of 'backpropagation' which solved this problem elegantly, with well-known mathematical tools.
However, backpropagation has a number of issues. It requires a symmetry between forward and backward weights, also known as the weight transport problem. It requires each layer to have full knowledge about its downstream layers before it can update its own weights, making it a non-local agorithm in the spatial as well as temporal sense.
Last but not the least, backpropagation is not really feasible in a biological sense. Our brain, which is still far superior to ML in cognitive tasks, can understand and interact with the environment without using backprop to train its synapses. This has led many researchers - including me - to believe that the next breakthrough in the field of ML will come from developing scalable learning algorithms that do not rely on backpropagation. And just maybe, they will even outperform backprop-based algorithms.
This folder contains my notes from papers on some of these alternate training algorithms for ML.
